{
    "contents" : "---\ntitle: \"Practical Machine Learning-  Final Project\"\nauthor: \"Jeffrey Uslan\"\ndate: \"Thursday, May 21, 2015\"\noutput: pdf_document\n---\n\nThe Project used the following packages:\n\n```{r}\nlibrary(zoo)\nlibrary(AppliedPredictiveModeling)\nlibrary(caret)\nlibrary(Hmisc)\nlibrary(Rcpp)\nlibrary(ggplot2)\nlibrary(rpart)\nlibrary(randomForest)\n```\n\nFirst, let's read in our training and testing sets.\n\n```{r}\ndata=read.csv(\"./pml-training.csv\")\n\nfinal_test=read.csv(\"./pml-testing.csv\")\n\n```\n\n Remove index variables\n```{r}\ndata=data[,-1]\nfinal_test=final_test[,-1]\n```\n\nIdentify which variables are viable as covariates. The code below identifies variables which are missing in the test set.\n\n\n```{r}\nNA_cols=apply(final_test,2,function(x){sum(is.na(x))==length(x)})\nNA_vars=names(final_test)[NA_cols]\n```\n\nRemove the missing variables from the training and test sets.\n\n```{r}\ndata=data[,!(names(data) %in% NA_vars)]\nfinal_test=final_test[,!(names(final_test) %in% NA_vars)]\n```\n\nWe can use the \"na.approx\" command from the \"zoo\" package to impute numeric variables.\n\n```{r}\nn_nums=sapply(data,is.numeric)\ndata[,n_nums]=na.approx(data[,n_nums])\n\nn_nums=sapply(final_test,is.numeric)\nfinal_test[,n_nums]=na.approx(final_test[,n_nums])\n\n```\n\nNext we identify variables in the training set with near zero variance. These variables would enough value to our predictions to justify the computation power required to process them. We also omit the timestamp variable at this time.\n\n```{r}\nNZV=nearZeroVar(data, saveMetrics=TRUE)\npoor_var=which(NZV$nzv)\nrm_vars=names(data)[c(which(names(data)==\"cvtd_timestamp\"),poor_var)]\n```\n\nRemove variables identified in the previous step from the training and test sets.\n\n```{r}\ndata=data[,!(names(data) %in% rm_vars)]\nfinal_test=final_test[,!(names(final_test) %in% rm_vars)]\n\n```\n\nKeep only numeric variables and the dependent factor \"classe\" for the training set. Remove incomplete observations from the training set.\n\n```{r}\nclasse_ind=which(names(data)==\"classe\")\nn_nums=which(sapply(data,is.numeric))\ndata=na.omit(data[,c(n_nums,classe_ind)])\n```\n\nKeep only numeric variables for the testing set.\n```{r}\nn_nums=which(sapply(final_test,is.numeric))\nfinal_test=final_test[,c(n_nums)]\n```\n\n\nSplit the the training set for cross validation.\n\n```{r}\ninTrain = createDataPartition(y=data$classe, p = .6)[[1]]\ntraining = data[ inTrain,]\ntesting = data[-inTrain,]\n```\n\nFit a random forest model on the sub-training set.\n\n```{r}\nmodFit <- randomForest(classe ~. , data=training)\n# modFit<-train(classe~.,data=training,method=\"rf\",prox=TRUE)\nprint(modFit)\n```\n\nThe out-of-sample error rate estimate is 0.17%\n\nCheck the in-sample error rate of the model on the sub-training set\n\n```{r}\npred=predict(modFit,newdata= training, type = \"class\")\nconfusionMatrix(training$classe, pred)\n```\n\nPredict the out-of-sample error rate of the model on the sub-testing set\n```{r}\npred=predict(modFit, testing, type = \"class\")\nconfusionMatrix(testing$classe,pred)\n```\n\nAccording to our cross-validation the out-of-sample error rate is 0.0008%\n\nApply the model to final test set.\n\n```{r}\npred=predict(modFit,newdata= final_test, type = \"class\")\n\npml_write_files = function(x){\n  n = length(x)\n  for(i in 1:n){\n    filename = paste0(\"problem_id_\",i,\".txt\")\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n  }\n}\n\npml_write_files(pred)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1437935647981.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3314778239",
    "id" : "1D99C335",
    "lastKnownWriteTime" : 1437935909,
    "path" : "~/Coursera_Data_Science_JH/class_08_Practical_Machine_Learning/cour_project_writeup.Rmd",
    "project_path" : "cour_project_writeup.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}